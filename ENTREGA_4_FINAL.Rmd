---
title: "ENTREGA_4_FINAL: Determinantes del desempleo juvenil en el mundo"
author: 'Integrantes: Joel B. Huamani y Fabian Rios'
author: "Jefe de práctica: Alexander Benites"
author: "Profesor: José Manuel Magallanes Reyes "
date: '2022-2'
subtitle: 'Curso: POL304 - Estadística para el análisis político 2'
output:
  prettydoc::html_pretty:
    theme: cayman
    highlight: github
---

Este breve reporte tiene como objetivo un análisis de correlación entre el desempleo juvenil (de jóvenes de 16 a 24 años) con las variables de la infraestructura de carreteras, la obesidad en adultos, la inversión extranjera, los gastos en educación pública y el PBI de 160 países del mundo en el año 2018. 
Esto se realizará por medio de la creación de modelos de regresión para advertir cuales de las variables presentadas tienen un impacto sobre la variable dependiente (Desempleo juvenil). Todo ello con el objetivo de entender en mayor profundidad algunos determinantes que podrían explicar el desempleo juvenil en el mundo. 

A continuación, se llevarán a cabo los siguientes análisis:

  - Realizar un análisis descriptivo de los resultados a nivel mundial

  - Implementar técnicas multivariadas para modelar el comportamiento del desempleojuvenil, particularmente de 18 a 24 años en el mundo

  - Aplicar técnicas de diagnósticos de regresión.
  
  - Aplicar técnicas de reducción de dimensionalidad
    
```{r include=FALSE}
#Librerías del ejercicio:
# include = FALSE para que no salga en el html.
library(rio)
library(dplyr)
library(ggplot2)
library(cowplot)
library(kableExtra)
library(reshape)
library(DT)
library(equatiomatic)
library(modelsummary)
library(factoextra)
library(cluster)
library(lmtest)
library(nortest)
```

```{r include=FALSE}
library(rio)
dataUNIFICADA="https://github.com/taiyonoJoel/LIMPIEZA/raw/main/dataunificada.csv"
datavar=import(dataUNIFICADA)

#Eliminando filas innecesarias:
datavar = datavar[-c(81:83, 133:139),]
```

```{r include=FALSE}
datavar$inverex       <- as.numeric(datavar$inverex)
datavar$PBI           <- as.numeric(datavar$PBI)
datavar$desempleojuv  <- as.numeric(datavar$desempleojuv)
```

```{r include=FALSE}
datavar$transito      <- as.numeric(datavar$transito)
datavar$obeadult      <- as.numeric(datavar$obeadult)
datavar$gastoseduc    <- as.numeric(datavar$gastoseduc)
```

```{r include=FALSE}
names(datavar)
str(datavar)
```

# Regresión Lineal Múltiple
```{r include=FALSE}
modelo1 = lm(desempleojuv ~ inverex + PBI, data=datavar)
summary(modelo1)
```

```{r include=FALSE}
modelo2 = lm(desempleojuv ~ inverex + PBI  +  transito +  obeadult + gastoseduc, data=datavar)
summary(modelo2)
```

Manteniendo todas las variables fijas...
(hipótesis)	
H0: No todos o ninguno de los indicadores presentan una significancia y no tienen un efecto relevante para explicar a la variable dependiente dentro de los modelos realizados
H1: Todos los indicadores muestran ser significativos y tienen un efecto relevante dentro de los modelos realizados.

+ Una vez revisamos los p-valores de cada modelo podemos observar que ambos modelos funcionan correctamente, lo que nos permite revisar cada modelo para confirmar que la hipotesis nula se ve rechazada. 

+ Observando ambos modelos, sin embargo, vemos que ûnicamente una de las variables se destaca como significante, en este caso, es la variable del crecimiento del PBI como se puede ver en el modelo 1 y 2. Este último se concentra en esa variable independiente. Entonces, con la hipótesis nula presente, tomamos en cuenta que tanto explican ambos modelos, 0.07087 y 0.06999, respectivamente. Estos valores son el porcentaje explicativo de cada modelo y vemos que hay una diferencia ínfima cuando vemos el cambio del primer modelo con el segundo: la adición de las demás variables: obesidad, gasto en educación y la infraestructura del tránsito no poseen un efecto significante en el desempleo juvenil a nivel mundial. 

+ Con estas conclusiones preliminares, vemos que los factores que toman en consideración Veronica Espinoza, para su revisión de la obesidad, como una barrera para el desempleo no se pueden aplicar en un marco a nivel global en comparación a los utilizados en Ecuador (a nivel nacional), de la misma manera ocurre en el Perú con Cesar Calvo y su postura con respecto a los gastos en educación y el ingreso recibido; los autores que realizaron sus investigaciones no es que no sean correctos, pero sus modelos de estudio no se pueden aplicar correctamente a nivel global.
  
```{r echo=FALSE}
equatiomatic::extract_eq(modelo2, use_coefs = TRUE)
```

```{r echo=FALSE}
model=list('desempleojuv'=datavar)
modelsummary(modelo2, title = "OLS",
             stars = TRUE,
             output = "kableExtra")
```

### Diagnósticos de regresión:

#### Linealidad:

```{r echo=FALSE}
plot(modelo2, 1)
```

Vemos que la distribución de los casos no sigue completamente la línea horizontal. Este modelo no tiene mucha linealidad, lo cual significa que este modelo no sería muy útil para hacer predicciones en otros casos.

#### Homocedasticidad:

```{r echo=FALSE}
plot(modelo2, 3)
```

```{r echo=FALSE}
bptest(modelo2)
```

El P-value es mayor a 0.05, por lo que se mantiene la hipótesis nula. Si hay homocedasticidad. El error del modelo no afecta su varianza.

#### Normalidad de los residuos:

```{r echo=FALSE}
plot(modelo2, 2)
```

```{r echo=FALSE}
shapiro.test(modelo2$residuals)
```

El P-value es menor a 0.05, por lo cual se rechaza la hipótesis nula. Los residuos del modelo no tiene una distribución normal.

#### No multicolinealidad:

Debido a que el modelo2 tiene solo una variable independiente, no esposible realizar una prueba de no multicolinealidad.

#### Valores influyentes:

```{r echo=FALSE}
plot(modelo2, 5)
```

Advertimos observaciones que están fuera de la línea de Cook, a saber, hay necesidad de eliminar algunos casos, ya que son problemáticos para el modelo2, a excepcion del PBI.

```{r echo=FALSE}
checkModelo2=as.data.frame(influence.measures(modelo2)$is.inf)
head(checkModelo2)
```

```{r echo=FALSE}
checkModelo2[checkModelo2$cook.d & checkModelo2$hat,]
```

Con las pruebas extras al modelo, podemos estar seguros de que hay casos que deberían eliminarse por su efecto en el modelo2. 


## Clustering:

Al graficar las variables, podemos ver que son bastante diferentes. Es muy difícil comparar variables que estén distribuidas de manera tan desigual. Para trabajar con ellas, estandarizaremos las variables.

```{r include=FALSE}
library(BBmisc)
```

```{r echo=FALSE}
boxplot(normalize(datavar[,-1],method='range',range=c(0,1)))
```

```{r echo=FALSE}
boxplot(normalize(datavar[,-1],method='standardize'))
```

  + Optamos por el método de standardize.

```{r include=FALSE}
datavar[,-1]=normalize(datavar[,-1],method='standardize') 
```

```{r echo=FALSE}
cor(datavar[,-1])
```

La variable del índice de (PBI) tiene una relación negativa con el resto de las variables. Esto puede ser un problema para el proceso de clusterización, por lo que invertiremos la variable PBI.

```{r echo=FALSE}
datavar$PBI=-1*datavar$PBI

#Veamos correlaciones entre estas variables tipificadas:
cor(datavar[,-1])
```

  + Con el índice de PBI invertido, la variable ahora muestra que tan NO desigual es un país.

```{r include=FALSE}
dataClus=datavar[,-1]
row.names(dataClus)=datavar$pais
```

```{r include=FALSE}
library(cluster)
g.dist = daisy(dataClus, metric="gower")
```

### Proponemos cantidad de clusters:

**PAM:**

```{r echo=FALSE}
library(factoextra)
fviz_nbclust(dataClus, pam,diss=g.dist,method = "gap_stat",k.max = 10,verbose = F)
```

**AGNES:**

```{r echo=FALSE}
fviz_nbclust(dataClus, hcut,diss=g.dist,method = "gap_stat",k.max = 10,verbose = F,hc_func = "agnes")
```

**DIANA:**

```{r echo=FALSE}
fviz_nbclust(dataClus, hcut,diss=g.dist,method = "gap_stat",k.max = 10,verbose = F,hc_func = "diana")
```

  - Optamos por formar 2 clusters

```{r include=FALSE}
#PAM
set.seed(123)
grupos=2
res.pam=pam(g.dist,k = grupos,cluster.only = F)
dataClus$pam=res.pam$cluster

#Agnes
res.agnes<- hcut(g.dist, k =grupos,hc_func='agnes',hc_method = "ward.D")
dataClus$agnes=res.agnes$cluster

#Diana
res.diana <- hcut(g.dist, k = grupos,hc_func='diana')
dataClus$diana=res.diana$cluster
```

### Análisis de las siluetas:

**PAM:**

```{r echo=FALSE}
fviz_silhouette(res.pam)
```

**AGNES:**

```{r echo=FALSE}
fviz_silhouette(res.agnes)
```

**DIANA:**

```{r echo=FALSE}
fviz_silhouette(res.diana)
```

Se observa que el método de Diana es el que tiene un silhouette width mayor que los otros métodos. Además, gracias al gráfico podemos observar que no tiene datos mal clusterizados. Por ello, optamos por utilizar Diana para hacer los clusters.

```{r}
library(magrittr)

silPAM=data.frame(res.pam$silinfo$widths)
silPAM$country=row.names(silPAM)
poorPAM=silPAM[silPAM$sil_width<0,'country']%>%sort()

silAGNES=data.frame(res.agnes$silinfo$widths)
silAGNES$country=row.names(silAGNES)
poorAGNES=silAGNES[silAGNES$sil_width<0,'country']%>%sort()

silDIANA=data.frame(res.diana$silinfo$widths)
silDIANA$country=row.names(silDIANA)
poorDIANA=silDIANA[silDIANA$sil_width<0,'country']%>%sort()
```

Podemos ver que PAM tiene 1 dato mal clusterizado, AGNES tiene 8 y DIANA, como vimos en el gráfico de siluetas, no tiene ninguno. Esto refureza nuestra decisión de utilizar este método, pues nos muestra que, al utilizar este método de clusterización, ningún país quedará fuera de los clusters que creemos.



```{r}
aggregate(.~ pam, data=dataClus,mean)
```

```{r}
original=aggregate(.~ pam, data=dataClus,mean)
original[order(original$inverex),]
```

```{r}
original[order(original$PBI),]
```

```{r}
original[order(original$desempleojuv),]
```

```{r}
original[order(original$transito),]
```

```{r}
original[order(original$obeadult),]
```

```{r}
original[order(original$gastoseduc),]
```

Vemos que en los dos clusters, los valores promedio de todas las variables son los mismos. En el cluster 1, se encuentran los casos con los menores valores en las variables. En el cluster 2, se encuentran los países con las variables con valores en promedio más altos.

```{r include=FALSE}
#Guardamos las columnas de PAM, AGNES Y DIANA en la data integrada, y eliminemosla de dataClus.

datavar$pamVARpoor=datavar$pais%in%poorPAM
datavar$pamVAR=as.ordered(dataClus$pam)
dataClus$pam=NULL

datavar$agnesVARpoor=datavar$pais%in%poorAGNES
datavar$agnesVAR=as.ordered(dataClus$agnes)
dataClus$agnes=NULL

datavar$dianaVARpoor=datavar$pais%in%poorDIANA
datavar$dianaVAR=as.ordered(dataClus$diana)
dataClus$diana=NULL
```


#### Graficamos los clusters:

```{r echo=FALSE,message=FALSE,warning=FALSE,eval=TRUE,fig.show='hold',fig.width=9.5,fig.height=5.55, fig.align="center"}
proyeccion = cmdscale(g.dist, k=2,add = T)

datavar$dim1 <- proyeccion$points[,1]
datavar$dim2 <- proyeccion$points[,2]

library(ggrepel)
base= ggplot(datavar,aes(x=dim1, y=dim2,label=row.names(dataClus))) 
base + geom_text_repel(size=3, max.overlaps = 50,min.segment.length = unit(0, 'lines'))
```

#### Gráfica de PAM
  - Conglomerados PAM en Mapa Bidimensonal de países
```{r echo=FALSE,message=FALSE,warning=FALSE,eval=TRUE,fig.show='hold',fig.width=9.5,fig.height=5.55, fig.align="center"}
# solo paises mal clusterizados
PAMlabels=ifelse(datavar$pamVARpoor,datavar$pais,'')

#base
base= ggplot(datavar,aes(x=dim1, y=dim2))  +
    scale_color_brewer(type = 'qual',palette ='Dark2'  ) + labs(subtitle = "Se destacan los países mal clusterizados")

pamPlot=base + geom_point(size=3, 
                          aes(color=pamVAR))  + 
        labs(title = "PAM") 
# hacer notorios los paises mal clusterizados
pamPlot + geom_text_repel(size=4,
                          aes(label=PAMlabels),
                          max.overlaps = 50,
                          min.segment.length = unit(0, 'lines'))
```
#### Gráfica de AGNES
  - Conglomerados Agnes en Mapa Bidimensional de países
```{r echo=FALSE,message=FALSE,warning=FALSE,eval=TRUE,fig.show='hold',fig.width=9.5,fig.height=5.55, fig.align="center"}
# solo paises mal clusterizados
AGNESlabels=ifelse(datavar$agnesVARpoor,datavar$pais,'')

agnesPlot=base + geom_point(size=3, 
                            aes(color=as.factor(agnesVAR))) +
          labs(title = "AGNES") 
# hacer notorios los paises mal clusterizados
agnesPlot + geom_text_repel(size=4,
                            aes(label=AGNESlabels),
                            max.overlaps = 50,
                            min.segment.length = unit(0, 'lines'))
```

#### Gráfica de DIANA
  - Conglomerados DIANA en Mapa Bidimensional de países
```{r echo=FALSE,message=FALSE,warning=FALSE,eval=TRUE,fig.show='hold',fig.width=9.5,fig.height=5.55, fig.align="center"}
# solo paises mal clusterizados
DIANAlabels=ifelse(datavar$dianaVARpoor,datavar$pais,'')

dianaPlot=base + geom_point(size=3,
                            aes(color=dianaVAR)) + 
          labs(title = "DIANA")

# hacer notorios los paises mal clusterizados
dianaPlot + geom_text_repel(size=4,
                            aes(label=DIANAlabels), 
                            max.overlaps = 50,
                            min.segment.length = unit(0, 'lines'))
```








```{r}
mapDIS=sf::read_sf("shapes_mundo.shp") #shapefile
```

```{r include=FALSE}
#Lo pasamos a la data original:
inputData$DISTRITO = row.names(inputData)
row.names(inputData) = NULL
inputData = inputData[,c(7,8)]
```


```{r include=FALSE}
data = merge(data, inputData, by = "DISTRITO")
```






```{r include=FALSE}
#Juntamos información con el shape:
mapDIS2=merge(mapDIS,data,by.x='IDDIST',by.y='UBIGEO', all.x = T) #siempre primero el shape
```




```{r include=FALSE}
#Lo pasamos a la data original:
inputData$DISTRITO = row.names(inputData)
row.names(inputData) = NULL
inputData = inputData[,c(7,8)]
```

```{r include=FALSE}
data = merge(data, inputData, by = "DISTRITO")
```

```{r include=FALSE}
data$clust = factor(data$clust, levels = c(1:3), labels = c("Vulnerabilidad alta","Vulnerabilidad baja","Vulnerabilidad media"))
```

Características de cada conglomerado:

```{r include=FALSE}
summ_clust = data %>% 
  group_by(clust) %>%
  summarise(IDH = mean(IDH_2019, na.rm=T),
            IVA = mean(IVA, na.rm = T),
            POBREZA = mean(POBREZA_2019, na.rm = T),
            EXT_POBREZA = mean(EXT_POBREZA_2019, na.rm =T),
            DISCAPACIDAD = mean(tDISC, na.rm = T),
            MAS80 = mean(tMAS80, na.rm = T))
```

```{r echo=FALSE}
summ_clust%>%
  kbl() %>%
  kable_minimal()
```

Número de distritos por cada conglomerado:

```{r include=FALSE}
bar1 = data %>%
  group_by(clust) %>%
  summarise(Cluster = n())
```

```{r echo=FALSE,message=FALSE,warning=FALSE,eval=TRUE,fig.show='hold',fig.width=7,fig.height=5, fig.align="center"}
ggplot(bar1, aes(x=clust, y=Cluster)) + 
  geom_bar(stat = "identity") +
  
  labs(title = " ",
       x = " ",
       y = " ") +
    geom_text(aes(label=Cluster), size=4, vjust=1.5, hjust = 0.5, color="white") + 
  
    theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(), panel.background = element_blank())
```

Análisis geoespacial de clusters:

```{r include=FALSE}
#Juntamos información con el shape:
mapDIS3=merge(mapDIS,data,by.x='IDDIST',by.y='UBIGEO', all.x = T) #siempre primero el shape
```

```{r echo=FALSE,message=FALSE,warning=FALSE,eval=TRUE,fig.show='hold',fig.width=9.5,fig.height=5.55, fig.align="center"}
mapaleyendaL= ggplot(mapDIS3)+ geom_sf() + theme_light()
mapaleyL= mapaleyendaL + geom_sf(data=mapDIS3,
              aes(fill=clust),color = "gray")
      
mapa1= mapaleyL +
coord_sf() + 
scale_fill_manual(values=c("tomato","turquoise","lightgoldenrod1")) + theme_void() +
  
  
theme(axis.title = element_blank(), axis.text = element_blank(), legend.position = "right") + labs(fill=" ") + theme(legend.text = element_text(size = 8)) +
  
labs(title = "Distritos según conglomerado de vulnerabilidad") +
  
theme(
plot.title = element_text(color="black", size=11, face="bold"))
mapa1
```













```{r}
library(sp)
library(rgdal)
```


```{r}
fromGit=("https://github.com/Alexanderbenit7/Test/blob/master/seattle.json?raw=true") #link desde github

wazipMap <- rgdal::readOGR(fromGit,stringsAsFactors = FALSE)
```

```{r}


library(ggplot2)
mapDIS=sf::read_sf("seattle.shp")
ggplot(mapDIS) + geom_sf()
```















